## 线性回归

一元线性回归问题：给定 $n$ 组数据 $(x_i,y_i)$ ，建立模型 $y_i=kx_i+b+\varepsilon$ ，其中 $\varepsilon$ 是误差项。

$f(k,b)=\underset{i=1}{\overset{n}{\sum}}(kx_i+b-y_i)^2$ 。

$\begin{cases}f_k=2\underset{i=1}{\overset{n}{\sum}}x_i(kx_i+b-y_i)=0\\ f_b=2\underset{i=1}{\overset{n}{\sum}}(kx_i+b-y_i)=0\end{cases}$



  多元线性回归：

$f(k_1,k_2,\cdots,k_p,b)=\underset{i=1}{\overset{n}{\sum}}(k_1x_{i1}+k_2x_{i2}+\cdots+k_px_{ip}+b-y_i)^2$ 。

$\begin{cases}f_{k_1}=2\underset{i=1}{\overset{n}{\sum}}x_{i1}(k_1x_{i1}+k_2x_{i2}+\cdots+k_px_{ip}+b-y_i)=0\\f_{k_2}=2\underset{i=1}{\overset{n}{\sum}}x_{i2}(k_1x_{i1}+k_2x_{i2}+\cdots+k_px_{ip}+b-y_i)=0\\ \vdots\\f_{k_p}=2\underset{i=1}{\overset{n}{\sum}}x_{ip}(k_1x_{i1}+k_2x_{i2}+\cdots+k_px_{ip}+b-y_i)=0\\f_b=2\underset{i=1}{\overset{n}{\sum}}(k_1x_{i1}+k_2x_{i2}+\cdots+k_px_{ip}+b-y_i)=0\end{cases}$



1、截距 $b$ 不是一个特殊的参数，我们可以构造虚拟变量 $x_{0}\equiv1$ ，这样 $b$ 就能够转化为 $x_0$ 的系数。

2、回归问题的参数估计可以转化成一个线性方程组，下面我们写成矩阵的形式。

$y=xb+\varepsilon$ ，$x_{n\times p},b_{p\times 1}$ 。

$L(b)=\varepsilon^{T}\varepsilon=(y-xb)^T(y-xb)$ 

$=y^Ty-y^Txb-b^Tx^Ty+b^Tx^Txb=b^Tx^Txb-2y^Txb+y^Ty$ 



$\frac{\partial L}{\partial b}=2(x^Txb-x^Ty)$ 

解为 $b=(x^Tx)^{-1}x^Ty$  



$L(b)$ 也叫残差平方和或 $RSS$ 。





实际操作：x,y可以进行变换 $x^k$ 。

线性回归能够处理单调的问题

$n<p$ 的问题称为高维问题。

稀疏化：就是让参数变得稀疏，让一些参数变成零。

在损失函数加上惩罚项 

$L(b)=\varepsilon^T \varepsilon+\lambda b^T b$        岭回归

这个优化问题有解析解，回去推一下。

$L(b)=\varepsilon^T \varepsilon+\lambda|b|$       Lasso回归，这是凸优化问题，证明。

以上两种方法，哪一个稀疏性更好，更容易使参数为零？





波士顿房价问题。Kaggle。